name: wikitext-103
source:
  hf_dataset: wikitext
  hf_config: wikitext-103-raw-v1
  split_map:
    train: train
    valid: validation
    test: test
  streaming: false
processing:
  text_field: text
  pack_sequences: true
  max_length: 512
  drop_remainder: true
limits:
  max_train_samples: null
  max_eval_samples: null
tokenizer:
  name: auto
  truncation: true
  padding: false
loader:
  batch_size: auto
  num_workers: ${compute.num_workers}
stats:
  compute_unigram_entropy: true
  compute_ngram_entropy: [2, 3, 4, 5, 6]
  compute_mi: true
