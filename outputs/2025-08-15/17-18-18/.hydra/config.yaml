compute:
  name: local_dev
  hardware:
    n_gpus: 1
    device: auto
    dtype: auto
    grad_checkpointing: true
    gradient_accumulation: auto
    max_batch_tokens: auto
    max_batch_size: auto
    num_workers: auto
    compile: true
  accelerate:
    dispatch_batches: true
    use_deepspeed: false
  profiling:
    log_every: 50
    eval_every_steps: auto
  budget:
    hours: auto
  gradient_accumulation: 1
  mixed_precision: bf16
  device: cuda
data:
  name: wikitext-103
  source:
    hf_dataset: wikitext
    hf_config: wikitext-103-raw-v1
    split_map:
      train: train
      valid: validation
      test: test
    streaming: false
  processing:
    text_field: text
    pack_sequences: true
    max_length: 512
    drop_remainder: true
  limits:
    max_train_samples: null
    max_eval_samples: null
  tokenizer:
    name: auto
    truncation: true
    padding: false
  loader:
    batch_size: 8
    num_workers: ${compute.num_workers}
    max_seq_len: 512
  stats:
    compute_unigram_entropy: true
    compute_ngram_entropy:
    - 8
    compute_mi: true
  ensure_bootstrap: true
model:
  name: bert-base-cased
  arch: mlm
  pretrained_name: bert-base-cased
  context_length: 512
  tokenizer: ${.pretrained_name}
  specialise:
    epochs: auto
    lr: auto
    weight_decay: 0.01
    warmup_ratio: auto
    scheduler: auto
    gradient_clip: 1.0
    freeze_embeddings: false
eval:
  name: crumpled_paper
  bpc:
    enabled: true
    compute_payload_bits: true
    compute_position_bits: true
  bpt:
    enabled: true
  amortised_bpc:
    enabled: true
    n_copies:
    - 1
    - 10
    - 100
    - 1000
    - 10000
  latency:
    enabled: true
    measure_decode_cpu: true
    measure_gpu_encode: true
    trials: 5
  flops:
    enabled: true
    mode: auto
  character_level:
    enabled: true
    use_levenshtein_when_lengths_differ: true
  chrf:
    enabled: true
    order: 6
  bertscore:
    enabled: true
    model_type: roberta-large
    batch_size: auto
  token_level:
    enabled: true
    only_when_tokenizers_match: true
  oracle:
    ensemble:
    - gpt2-xl
    - google/gemma-3-270m
    measurement_tokenizer: gpt2-xl
  alignment:
    algorithm: needleman_wunsch
    gap_penalty_bits: auto
  metrics:
    tcm: true
    pcm: true
    delta_ll_bits: true
  semantic_fidelity:
    enabled: true
    sbert_model: sentence-transformers/all-mpnet-base-v2
    span: auto
experiment:
  name: e1a_wiki103
  seeds: 5
  static_bits_override: 0
  model_groups:
    mlm:
    - bert-base-cased
    - roberta-base
    - distilroberta
    ar:
    - gpt2
    - gpt2-medium
    - gpt2-large
logging:
  wandb:
    enabled: false
    project: lldc
    entity: null
stage1:
  masking:
    policy: targeted_dynamic
    policy_set_fraction: 0.1
    pll_token_scoring: true
    recompute_each_epoch: true
    curriculum:
      rate_start: 0.2
      rate_end: 0.8
      schedule: auto
  train:
    epochs: auto
    lr: auto
    scheduler: auto
    warmup_ratio: auto
    batch_size: ${data.loader.batch_size}
    gradient_accumulation: ${compute.gradient_accumulation}
    grad_clip: 1.0
    save_best: true
  apply_to: mlm
  models: ${model_groups.mlm}
stage2:
  pm:
    enabled: true
    strategies:
    - topk_global
    - entropy_equalisation
    mask_rates:
    - 0.2
    - 0.4
    - 0.6
    - 0.8
    position_codec: auto
    arithmetic_coder:
      enabled: true
      oracle_model: gpt2-large
      kept_only_loop: true
    models: ${model_groups.mlm}
  vq:
    enabled: true
    codebook_sizes:
    - 256
    - 512
    - 1024
    - 2048
    - 4096
    - 8192
    reuse_cache: true
    force_retrain: false
    bottleneck:
      layer_after: 6
      shared_codebook: true
      commitment_beta: 0.25
    index_lm:
      type: gru
      layers: 1
      hidden_size: 512
      epochs: 2
    models: ${model_groups.ar}
stage3:
  stopping:
    eos_or_maxlen_margin: 1.05
evaluation:
  run_efficiency: true
  run_fidelity: true
  run_crumpled_paper: true
crumpled_paper:
  oracle_models:
  - gpt2-large
  measuring_tokenizer: oracle
  alignment:
    algorithm: needleman_wunsch
    gap_penalty_bits: auto
  spans:
    sbert_model: sentence-transformers/all-mpnet-base-v2
e2a:
  pruning:
    method: structured
    schedule:
      levels:
      - 0.0
      - 0.1
      - 0.2
      - 0.3
      - 0.4
      - 0.5
      order: auto
    structured:
      drop_attention_heads: true
      drop_ffn_channels: true
      drop_layers: false
    saliency_metric: magnitude
  recovery:
    finetune_epochs: auto
    lr: auto
    scheduler: auto
    warmup_ratio: auto
  stage2_reuse: true
e2b:
  deduplication:
    ngram:
      n_list:
      - 3
      - 4
      - 5
      top_k: 10
    semantic:
      minhash_bands: 20
      minhash_rows: 5
      sbert_model: sentence-transformers/all-mpnet-base-v2
  regen_baseline:
    subsample_every_N:
    - 3
    - 5
    - 7
    kenlm_workdir: artifacts/runs/kenlm_5gram
    beam_size: 16
    cand_per_step: 200
    max_vocab: 50000
  scalability:
    dataset: wikitext-2
    factors:
    - 1
    - 2
    - 4
    - 8
    - 16
    - 32
    temperature_protocol: fixed_greedy
  outputs:
    write_rd_curves: true
    write_tables: true
outputs:
  write_rd_curves: true
  write_tables: true
  save_payloads: true
name: e1a_wiki103
model_groups:
  mlm:
  - roberta-base
  - bert-base-cased
  - distilroberta
  ar:
  - gpt2
  - gpt2-medium
  - gpt2-large
replication:
  seeds: auto
  aggregate: mean_std
seed: 13
